---
title: Class 12
date: February 17, 2023
---

# Recap

\begin{equation}
V_\alpha (s) = \max_a \bc{r(s,a) + \alpha \sum P(j | s,a) V_\alpha(j)}
\label{eq1}
\end{equation}

\begin{equation}
V_f (s) = r(s,f(s)) + \alpha \sum P(j | s, f(s)) V_f(j)
\end{equation}


\begin{equation}
T_\alpha u (s) = \max_a \bc{ r(s,a) + \alpha \sum P(j | s, a) u(j)}
\end{equation}


\begin{equation}
T_f u(s) = r(s,f(s)) + \alpha \sum P(j | s, f(s)) u(j)
\end{equation}

Essentially, Policy Iteration can be approximated to Value Iteration algorithm by

$$
\lim_{n\ra \far} T^n_\alpha u = V_\alpha
$$

\begin{algorithm}
\begin{enumerate}
  \item Start with an arbitrary initial vector $u \in B(\mathcal{S})$  and set $n =0$.

\item  For each $s$ find $V_{n+1}(s)$ using \eqref{eq1}.

\item  If $\|V_{n+1} - V_n\| \le \epsilon$ for all states then stop. Else repeat for the next $n$.

  \item Then get policy using argmax.
\end{enumerate}

  \caption{Value Iteration}
\end{algorithm}

# Bounds on $\| V_n - V_\alpha\|$ 

We already know $\| V_n - V_\alpha\| \le \alpha^n \| V_0 - V_\alpha\|$ but this is not useful. 

Similarly, $\| V_n - V_{n+1}\| \le \alpha^n \| V_0 - V_1\|$

So we want to obtain a bound on $\| V_n - V_\alpha\|$ in terms of $\| V_0 - V_1\|$. We'll see how this helps.

Using Triangle Inequality,
\begin{align*}
  \| V_n - V_\alpha\| &= \|(V_n - V_{n+1}) + (V_{n+1} - V_{n+2}) + \ldots (V_{n+l}- V_\alpha)\|\\
  &\le \alpha^n \|V_1 - V_0\| (1 + \alpha  + \alpha^2 \ldots + \alpha^{l - 1}) + \|V_{n+l} - V_\alpha \|\\
  &\le \frac{\alpha^n}{1 - \alpha} \|V_1 - V_0\| \quad \text{Setting } l \ra \far
\end{align*}

## Stopping Criteria

$$
\|V_n - V_\alpha \| \le \frac{\alpha}{1 - \alpha} \|V_n - V_{n-1}\|
$$ 

\begin{proof}
  \begin{align*}
\|V_n - V_\alpha \| &\le \|V_n - V_{n+1}\| + \| V_{n+1} - V_\alpha\| \\
&\le \alpha \|V_{n-1} - V_{n}\| + \alpha \| V_n - V_\alpha \|
  \end{align*}
\end{proof}

Now, assume we want to stop at a $\delta$ where $\|V_n - V_\alpha\| \le \delta$.

That means $\frac{\alpha}{1 - \alpha} \|V_n - V_{n-1}\|\le \frac{\alpha \epsilon}{1 - \alpha}$ which gives us $\delta = \frac{\alpha \epsilon}{1 - \alpha}$.

# Gaus-Seidel or In-Place (Asynchronous) Value Iteration

Don't keep separate vectors for $V_{n}$ and $V_{n+1}$. Just solve them in a single vector $V$.

# Modified Policy Iteration

Taking something from both Policy and Value Iteration. VI is faster per iteration than PI but PI takes less iterations.

Essentially don't evaluate $V_{\pi_n}$ fully for an intermediate policy $\pi_n$. 


\begin{algorithm}

\begin{enumerate}
  \item Set $n=0$ and arbitrary $V_0$ and find $\pi_0$ that is greedy wrt $V_0$
  \item (Partial Policy Evaluation): Obtain $V_n$ by repeatedly appling $T_{\pi_n}$ on $V_{n-1}$ for $m_n$ number of times.
  \item (Greedy Step): Find policy $\pi^{n+1}$ that is greedy on $V_n$.
  \item If $\| V_{n+1} - V_n\| \le \epsilon$ then STOP.
\end{enumerate}

  \caption{Modified PI}
\end{algorithm}

When $m_n = 1$ then this algorithm gives us VI. 
