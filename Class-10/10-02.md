---
title: Class 10
date: February 10, 2023
---

# Recap

- $\alpha$-optimal policy
- $V_\alpha$ policy and associated Bellman Equation

# Ross' Notation

- $B(\mathcal{S})$: Set of all bounded (real-valued) Functions on the state space
- $u \in B(\mathcal{S})$
- Map $T_f: B(\mathcal{S}) \rightarrow B(\mathcal{S} )$  for policy vector $f$ over the state space. Each $f(i)$ belongs to the action space.

$$
(T_f u)(i) = r(i, f(i)) + \alpha \sum_{j=0}^\infty P_{ij} (f(i)) u(j)
$$ 
Intuition for the map is the policy evaluation equation:

$$
V^\pi(s) = r(s, \pi(s)) + \alpha E_{s,\pi} \bs{V^\pi (S')}
$$ 

- **Terminal Reward**: Just the reward of terminating at the state (doesn't consider the immediate reward).

So, interpretation of $T_f u$ has the interpretation of following policy $f$ for one step before terminating with terminal reward $\alpha u$ ($\alpha u(j)$ when final state is $j$).

\begin{definition}
  $T_f^n = T_f (T_f^{n-1})$ or $T_f(T_f(T_f( \ldots )))$ (n times).
\end{definition}

\begin{definition}
  For any two functions $u,v,u_n \in B(\mathcal{S})$:

  \begin{enumerate}
    \item $u \le v$ if $u(i) \le v(i)$ for all $i$
    \item $u = v$ if $u(i) = v(i)$ for all $i$
    \item $u_n \rightarrow u$ if $u_n(i) \rightarrow u(i)$ for all $i$
  \end{enumerate}
\end{definition}

\begin{lemma}
  For $u, v \in B(\mathcal{S})$ and a stationary policy $f$

  \begin{enumerate}
    \item $u \le v \implies T_f u \le T_f v$ 
    \item $T_f V^f = V^f$ 
    \item $T_f^n u \rightarrow V^f,\ \forall u \in B(\mathcal{S})$
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item Easy to proof
    \item Place revenue back in the conditioned expectation and it will be same as $V_f$
    \item $T^n_f u$ is following $f$ for $n$ steps and obtaining a terminal reward of $\alpha^n u$ and then taking $n\rightarrow \infty$. Basically, $T_f^n u$ indiciates $n$-period value function and as $n\rightarrow \infty$, this will become the infinite horizon value function.
  \end{enumerate}
\end{proof}

# Policy Evaluation Algorithm

- For a policy $f$, keep applying $T_f$ and you'll get $V_f$

<!-- TODO: -->

# Underlying MRP under $f$ 

\begin{remark}
Markov Reward Process: No action
\end{remark} 

The *Policy Evaluation* equation can be rewritten as

$$
V^f = r^f + \alpha P^f V^f
$$

# Optimal stationary policy $f_\alpha$

Let's say you have the optimal stationary policy $f_\alpha$. 

\begin{theorem}

$$
V^{f_\alpha} (i) = V_\alpha (i) \ \forall i \ge 0
$$ 

and hence $f_\alpha$ is optimal

\end{theorem}

\begin{proof}
  Apply $T_{f_\alpha}$ operator to $V_\alpha$. This gives us $T_{f_\alpha} V_\alpha = V_\alpha$. Now repeatedly apply. This gives $\lim_{n\ra \far} T^n_{f_\alpha} = V_{f_\alpha} = V_\alpha$

  % TODO:
\end{proof}

# Improving a policy $f$ 


